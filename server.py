from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from langchain_community.llms import Ollama
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from pathlib import Path
import hashlib, json, os

app = Flask(__name__)
CORS(app)

# === é…ç½®è·¯å¾„ ===
doc_folder = "documents"
faiss_index_path = "faiss_index"
hash_log_path = "processed_hashes.json"

# === åµŒå…¥æ¨¡å‹ä¸åˆ‡åˆ†å™¨ ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)

# === å“ˆå¸Œå·¥å…· ===
def file_hash(path: Path):
    return hashlib.md5(path.read_bytes()).hexdigest()

def load_hash_log():
    return json.loads(Path(hash_log_path).read_text()) if Path(hash_log_path).exists() else {}

def save_hash_log(log):
    Path(hash_log_path).write_text(json.dumps(log, indent=2), encoding="utf-8")

def load_new_documents():
    docs, prev, curr = [], load_hash_log(), {}
    for file in Path(doc_folder).glob("**/*"):
        if file.suffix.lower() not in [".txt", ".md", ".pdf"]:
            continue
        try:
            fid = file_hash(file)
            curr[str(file)] = fid
            if prev.get(str(file)) == fid:
                continue
            loader = TextLoader(str(file), encoding="utf-8") if file.suffix != ".pdf" else PyPDFLoader(str(file))
            for d in loader.load():
                d.metadata["source"] = str(file)
                docs.append(d)
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ï¼š{file.name}ï¼š{e}")
    save_hash_log(curr)
    return docs

# === åˆå§‹åŒ–å‘é‡åº“ ===
if Path(faiss_index_path).exists():
    print("ğŸ“¦ åŠ è½½å·²æœ‰å‘é‡åº“...")
    vs = FAISS.load_local(faiss_index_path, embedding_model, allow_dangerous_deserialization=True)
    new_docs = load_new_documents()
    if new_docs:
        vs.add_documents(text_splitter.split_documents(new_docs))
        vs.save_local(faiss_index_path)
else:
    print("ğŸ§± æ„å»ºæ–°å‘é‡åº“...")
    docs = text_splitter.split_documents(load_new_documents())
    vs = FAISS.from_documents(docs, embedding_model)
    vs.save_local(faiss_index_path)

llm = Ollama(model="deepseek-r1:7b")
retriever = vs.as_retriever()

# === ä¿ç•™è¿è¡Œæ—¶è®°å¿†ï¼ˆä»…å†…å­˜ã€3è½®ï¼‰
chat_history = []

# server.py ä¿®æ”¹åçš„ chat æ¥å£éƒ¨åˆ†ï¼ˆå…¶ä½™å†…å®¹ä¸å˜ï¼‰
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.get_json()
    query = data.get("question", "").strip()
    if not query:
        return jsonify({"answer": "â—é—®é¢˜ä¸èƒ½ä¸ºç©º"}), 400

    retrieved = retriever.get_relevant_documents(query)
    doc_context = "\n".join([d.page_content.strip() for d in retrieved[:3]])

    context_prompt = ""
    for item in chat_history[-3:]:
        context_prompt += f"ç”¨æˆ·ï¼š{item['question']}\nAIï¼š{item['answer']}\n"

    final_prompt = ""
    if doc_context:
        final_prompt += f"ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„å‚è€ƒèµ„æ–™ï¼š\n{doc_context}\n\n"
    final_prompt += context_prompt + f"ç”¨æˆ·ï¼š{query}\nAIï¼š"

    full_answer = llm.invoke(final_prompt)

    # åˆ†ç¦» <think> æ ‡ç­¾å†…å®¹
    import re
    match = re.search(r"<think>(.*?)</think>(.*)", full_answer, re.DOTALL)
    if match:
        thinking = match.group(1).strip()
        answer = match.group(2).strip()
    else:
        thinking = ""
        answer = full_answer.strip()

    chat_history.append({"question": query, "answer": full_answer})
    return jsonify({
        "thinking": thinking,
        "answer": answer
    })

@app.route("/")
def serve_index():
    return send_from_directory(".", "index.html")

if __name__ == "__main__":
    app.run(port=5000)
