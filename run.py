from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from pathlib import Path
import hashlib
import json
import os

# === è·¯å¾„è®¾ç½® ===
doc_folder = "documents"
faiss_index_path = "faiss_index"
hash_log_path = "processed_hashes.json"

# === åµŒå…¥æ¨¡å‹å’Œæ–‡æœ¬åˆ‡åˆ†å™¨ ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)

# === å“ˆå¸Œå‡½æ•°ç”¨äºæ–‡ä»¶å»é‡ ===
def file_hash(file_path: Path) -> str:
    hasher = hashlib.md5()
    with open(file_path, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()

# === è¯»å–/ä¿å­˜ æ–‡æ¡£å“ˆå¸Œè®°å½• ===
def load_hash_log():
    if Path(hash_log_path).exists():
        return json.loads(Path(hash_log_path).read_text())
    return {}

def save_hash_log(hash_log):
    with open(hash_log_path, "w", encoding="utf-8") as f:
        json.dump(hash_log, f, indent=2)

# === è½½å…¥æ–‡æ¡£ ===
def load_new_or_changed_documents():
    all_docs = []
    previous_hashes = load_hash_log()
    current_hashes = {}
    
    for file_path in Path(doc_folder).glob("**/*"):
        suffix = file_path.suffix.lower()
        if suffix not in [".txt", ".md", ".pdf"]:
            continue

        try:
            file_id = file_hash(file_path)
            current_hashes[str(file_path)] = file_id

            if previous_hashes.get(str(file_path)) == file_id:
                continue  # æœªå˜æ›´ï¼Œè·³è¿‡

            if suffix in [".txt", ".md"]:
                loader = TextLoader(str(file_path), encoding="utf-8")
            else:
                loader = PyPDFLoader(str(file_path))

            docs = loader.load()
            for doc in docs:
                doc.metadata["source"] = str(file_path)
            all_docs.extend(docs)

        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ï¼š{file_path.name}ï¼ŒåŸå› ï¼š{e}")

    save_hash_log(current_hashes)
    return all_docs

# === è½½å…¥æˆ–æ„å»ºå‘é‡åº“ ===
if Path(faiss_index_path).exists():
    print("ğŸ“¦ åŠ è½½å·²æœ‰å‘é‡åº“...")
    vector_store = FAISS.load_local(
        faiss_index_path,
        embedding_model,
        allow_dangerous_deserialization=True
    )
    print("ğŸ“„ æ£€æŸ¥æ˜¯å¦æœ‰æ–°å¢æ–‡æ¡£...")
    new_docs = load_new_or_changed_documents()
    if new_docs:
        split_docs = text_splitter.split_documents(new_docs)
        vector_store.add_documents(split_docs)
        vector_store.save_local(faiss_index_path)
else:
    print("ğŸ§± æ­£åœ¨æ„å»ºæ–°å‘é‡åº“...")
    all_docs = load_new_or_changed_documents()
    split_docs = text_splitter.split_documents(all_docs)
    vector_store = FAISS.from_documents(split_docs, embedding_model)
    vector_store.save_local(faiss_index_path)

# === åˆå§‹åŒ–æ¨¡å‹å’ŒQAé“¾ ===
llm = Ollama(model="deepseek-r1:7b")
retriever = vector_store.as_retriever()
qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

# === å½“å‰è¿è¡Œä¸­çš„è®°å¿†ï¼ˆåªä¿ç•™æœ€è¿‘ 3 è½®ï¼‰ ===
chat_history = []

# === å¯åŠ¨äº¤äº’ ===
print("ğŸ” è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼Œè¾“å…¥ 'exit' é€€å‡ºï¼Œè¾“å…¥ 'reset' æ¸…ç©ºè®°å¿†ï¼š")
while True:
    query = input("ä½ ï¼š").strip()
    if query.lower() in ["exit", "quit"]:
        print("ğŸ‘‹ å†è§ï¼")
        break
    if query.lower() == "reset":
        chat_history = []
        print("ğŸ§  å·²æ¸…ç©ºè®°å¿†ã€‚")
        continue

    # æ–‡æ¡£æ£€ç´¢ï¼ˆä»ç„¶åŸºäº query æœ¬èº«ï¼‰
    retrieved_docs = retriever.get_relevant_documents(query)
    # print("\nğŸ“ å‘½ä¸­å‚è€ƒç‰‡æ®µï¼š")
    # shown = False
    # for i, doc in enumerate(retrieved_docs):
    #     content = doc.page_content.strip()
    #     if not content:
    #         continue
    #     source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
    #     print(f"ã€æ–‡æœ¬{i+1}ã€‘(æ¥è‡ªï¼š{Path(source).name})\n{content}\n")
    #     shown = True
    # if not shown:
    #     print("ï¼ˆæœªæ‰¾åˆ°æ˜æ˜¾ç›¸å…³çš„ç‰‡æ®µï¼‰")

    # æ‹¼æ¥è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆä»…æœ¬è½®è¿è¡Œçš„æœ€å 3 è½®ï¼‰
    context_prompt = ""
    for item in chat_history[-3:]:
        context_prompt += f"ç”¨æˆ·ï¼š{item['question']}\nAIï¼š{item['answer']}\n"

    # æ‹¼æ¥æ–‡æ¡£å†…å®¹
    doc_context = "\n".join([doc.page_content.strip() for doc in retrieved_docs[:3]])
    final_prompt = ""
    if doc_context:
        final_prompt += f"ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„å‚è€ƒèµ„æ–™ï¼š\n{doc_context}\n\n"
    final_prompt += context_prompt + f"ç”¨æˆ·ï¼š{query}\nAIï¼š"

    # è·å–å›ç­”
    print("\nğŸ¤– AI å›ç­”ï¼š")
    answer = llm.invoke(final_prompt)
    print(answer)

    # åªä¿ç•™æœ¬è½®è¿è¡Œä¸­çš„å¯¹è¯å†å²ï¼ˆå†…å­˜ä¸­ï¼‰
    chat_history.append({"question": query, "answer": answer})
